\chapter{Literature Study}
% put these two lines after every \chapter{} command
\vspace{-2em}
\minitoc

\startarabicpagenumbering % must be just after the first \chapter{} command


The implementation of FDIR on satellites have multiple complications with regards to the type of data generated by a satellite and the methodologies that can be implemented within the time and memory constraint of a cube-sat processor.

\section{Satellites}

\subsection{Typical Faults}
For the simulation of the satellite and the induced faults to train and test various anomaly detection methodologies a database of typical faults is required. Towards this purpose a database of typical faults were generated based on study by \textcite{tafazoli2009study}. 

\subsection{ADCS}

\subsubsection{Typical Faults}
A set of typical faults for the ADCS is shown in Table~\ref{ADCS faults}.


\section{Anomaly Detection On Satellites}
Various methodologies have been tested on different component of satellites. Therefore a summary of these research articles are provided in this section.

\subsection{FDR of gyroscope's drift}
\cite{carvajal2017agent}

\section{Anomaly detection methods}

\subsection{Statistical Methods}
\subsubsection{Pearson Correlation}
Vectors of certain sensors are highly correlated. For instance the vector of the earth sensor is highly correlated since the magnitude of the vector remains more or less constant. To detect anomalies the correlation of vectors can be measured and with a specified threshold the correlation can be indicated as a anomaly or nor.

The squared Pearson correlation coefficient (SPCC) for vectors depicted as
\linebreak
\\
\centerline{$a = [a_1, a_2, \ldots, a_L]^T,$}
\linebreak
\centerline{$b = [b_1, b_2, \ldots, b_L]^T,$}
\\
is defined as \cite{benesty2009pearson}
\begin{equation}
\rho^2 (a,b) = \frac{E^2 (a,b)}{E(a^Ta)E(b^Tb)}.
\end{equation}
The correlation coefficient is proven to be constraint as
\begin{equation}
0 \leq \rho \leq 1,
\end{equation}
where $\rho = 1$ is perfect linear correlation. 

\subsubsection{Kernel Canonical Correlation Analysis}
Due to the orbital nature of satellites there exist a correlation between various sensors. For instance the sun sensor, magnetometer and earth sensor are correlated based on the desired orientation and orbit of the satellite. This correlation might not be of linear nature, but with non-linear correlation methods such as kernel canonical correlation the correlation can be measured.

\cite{fukumizu2007statistical}

\subsubsection{Variance}
Within a sequential data sample of the satellite, the variance of the variables should be within a given threshold if the satellite is in a stable condition. The variance of the data sample is defined as 
\begin{equation}
S^2 = \frac{\sum(x_i + \bar{x})^2}{n-1}
\end{equation}
where $x$ defines the variable within the dataset.

\subsubsection{Kalman-Filter}
The Kalman-filter application would require the state-space matrices to be provided in the log file.

\subsubsection{Multivariate Guassian Distribution}
The assumption that the error of our data is generated with a Guassian distribution with a specific mean, $\mu$, and variance, $\sigma^2$, provides the opportunity for using multi-variate Gaussian distribution to determine the probability of a data-sample within a dataset. 
\begin{equation}
\label{mean}
\mu_j = \frac{1}{m} \sum_{i=1}^{m}x_j^{(i)}
\end{equation}

\begin{equation}
\label{variance}
\sigma_j^2 = \frac{1}{m} \sum_{i=1}^{m}(x_j^{(i)} - \mu_j)^2
\end{equation}

\begin{equation}
\label{guassian distribution}
p(x) = \prod_{j=1}^{n} \frac{1}{\sqrt{2\pi}\sigma_j}exp(-\frac{(x_j-\mu_j)^2}{2\sigma_j^2})
\end{equation}

For multi-variate Guassian distribution \cite{do2008multivariate}.

\begin{equation}
\label{sum}
\sum = \frac{1}{m}\sum_{i=1}^{m}(x^{(i)}-\mu)(x^{(i)}-\mu)^T
\end{equation}

\begin{equation}
\label{multi-variate guassian distribution}
p(x) = \frac{1}{(2\pi)^{\frac{n}{2}}{\lvert \sum \rvert}^\frac{1}{2}} exp(-\frac{1}{2}(x-\mu)^T{\sum}^{-1}(x-\mu))
\end{equation}

The Anomalies will be classified based on probabilities smaller than a given threshold.

\begin{algorithm}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}

\SetKwData{Left}{left}
\SetKwData{This}{this}
\SetKwData{Up}{up}
\SetKwFunction{Union}{Union}
\SetKwFunction{FindCompress}{FindCompress}

\Indm
\Input{Data sample from satellite orbit.}
\Output{Whether dataset contains anomaly.}
\Indp
\BlankLine

Determine feature vectors $x_i$ \\
Determine threshold probabilty, $\epsilon$ \\
Calculate $\mu_j$ with Eq~\ref{mean} \\
Calculate $\sigma_j$ with Eq~\ref{variance} \\
Calculate $p(x)$ with Eq~\ref{guassian distribution} \\
\If{$p(x) < \epsilon$}{Anomaly $= True$}
\Else{Anomaly $= False$}

\caption[Multi-variate Guassian Distribution]{Multi-variate Guassian Distribution Algorithm}
\label{alg}
\end{algorithm}

\subsubsection{Kullback-Leibler Divergence}
The Kullback-Leibler divergence quantifies the difference between two probability density functions, denoted as $p(x)$ and $q(x)$ \cite{hershey2007approximating}. Satellites are systems that are predictable within a time-series. The divergence between two sequential data buffers from the satellite will have a very similar probability distribution. Therefore calculating the difference between two datasets can be used to detect an anomaly based on a given threshold.

The difference between the Guassian distributions from datasets, $a$ and $b$, in Figure~\ref{Guassian plot} cannot simply be calculated as the difference in the mean or the difference in the variance. To overcome this, the divergence between the two distributions can be calculated. Intuitively a point $x$ with a high probability in the dataset $a$ should have a high probability in the dataset $b$ if the two datasets have a small divergence. 

\pgfmathdeclarefunction{gauss}{3}{%
	\pgfmathparse{1/(#3*sqrt(2*pi))*exp(-((#1-#2)^2)/(2*#3^2))}%
}
\begin{figure}[!h]
\centering
\textbf{Difference between Guassian Distributions}
\begin{tikzpicture}
	\begin{axis}[
		no markers, 
		domain=-3:6, 
		samples=100,
		ymin=0,
		axis lines*=left, 
		xlabel=$x$,
		every axis y label/.style={at=(current axis.above origin),anchor=south},
		every axis x label/.style={at=(current axis.right of origin),anchor=west},
		height=5cm, 
		width=12cm,
		xtick=\empty, 
		ytick=\empty,
		enlargelimits=false, 
		clip=false, 
		axis on top,
		grid = major,
		hide y axis
		]
		
		\addplot [very thick,cyan!50!black] {gauss(x, 3, 1)};
		
		\pgfmathsetmacro\valueA{gauss(3,3,1)}
		\draw [gray] (axis cs:3,0) -- (axis cs:3,\valueA);

		\node[below] at (axis cs:3, 0)  {$\mu_p$}; 
		
		\addplot [very thick,red!50!black] {gauss(x, 1.5, 1.5)};
		
		\pgfmathsetmacro\valueB{gauss(1.5,1.5,1.5)}
		\draw [gray] (axis cs:1.5,0) -- (axis cs:1.5,\valueB);
		
		\node[below] at (axis cs:1.5, 0)  {$\mu_q$}; 
	\end{axis}
	
\end{tikzpicture}
\caption{Guassian Distributions}
\label{Guassian plot}
\end{figure}

The divergence can be expressed as 

\begin{equation}
KL(P\lvert\lvert Q) = \int p(x) \log \left( \frac{q(x)}{p(x)} \right)dx.
\end{equation}

\subsection{Supervised Learning}
Supervised Learning consists of models that are trained on labelled data. This is not a problem with simulation, but with the real data, it is a problem and to provide tests on the real data to label it must be proficient. If unsupervised learning and statistical methods are not sufficient in their accuracy, a method for labelling the real data must be provided.

Time-series data: LSTM or DLSTM

Support Vector Machines
Naive Bayes
K-nearest neighbours

Artificial Neural Networks

\subsection{Unsupervised Learning}
Isolation Forests: Are based on the principle of randomly dividing a dataset. The data points that are closer to the root of the division is anomalies. Isolation forest are small in memory and are fast in computing anomalies.

Feature extraction: Prony's method. Convolutional networks can also be used for feature extraction.

LOF: Local outlier factor is a method of determining how much an outlier a specific data point is relative to a neighbourhood of other data points.

K-clustering: Clustering multiple points with similar features.

Kernel adaptive density-based: Is an algorithm that uses the density factor of a data point relative to other data points to determine whether the data point is an outlier or not.

Loda: Is a fast and efficient anomaly detection algorithm that used histograms to evaluate data points to determine whether a data point is an outlier. Loda is an on-line method and not a batch method.

\subsection{Reinforcement Learning}
Active Anomaly detection with meta-policy (Meta-AAD) is a deep reinforcement learning approcah that is based on the actor-critic model. The agent must query data points within the given dataset (where the queried point is the data top 1 data point). The query is given to a human 

\section{Summary}

